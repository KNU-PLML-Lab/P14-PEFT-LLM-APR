import time
import traceback
import torch
import torch.nn as nn
import tqdm
from dataset import Dataset, custom_collate
from transformers import (
  AutoTokenizer,
  CodeGenForCausalLM,
  get_cosine_schedule_with_warmup,
  Adafactor
)

def step_validation(model, device_ids, validation_loader, save_dir, parallel=False):
  print('üß´ Validation...')
  validation_loss = []
  # no_gradÎ•º ÏÑ§Ï†ïÌï¥ validation Í≥ºÏ†ïÏóêÏÑú gradientÍ∞Ä Í≥ÑÏÇ∞ÎêòÏßÄ ÏïäÎèÑÎ°ù Ìï®
  with torch.no_grad():
    for _, data in enumerate(validation_loader):
      data = {
        'input_ids': data['input_ids'].to(device_ids[0]),
        'labels': data['labels'].to(device_ids[0]),
        'attention_mask': data['attention_mask'].to(device_ids[0])
      }
      output = model(
        input_ids=data['input_ids'],
        labels=data['labels'],
        attention_mask=data['attention_mask'],
        return_dict=True
      )
      loss = output.loss
      # lossÏùò ÌèâÍ∑†ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ validation_lossÏóê Ï∂îÍ∞Ä
      validation_loss.append(loss.mean().item())
  # validation_lossÏùò ÌèâÍ∑†ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ Ï∂úÎ†•
  print('üî¨ Validation loss:', round(sum(validation_loss) / len(validation_loss), 4))
  # Î™®Îç∏ Ï†ÄÏû•
  if not parallel:
    model.module.save_pretrained(save_dir)
  else:
    model.save_pretrained(save_dir)
  # Î™®Îç∏ÏùÑ Îã§Ïãú train Î™®ÎìúÎ°ú Ï†ÑÌôò
  model.train()

def run_finetune(
  training_file, # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌååÏùº Í≤ΩÎ°ú
  validation_file, # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÌååÏùº Í≤ΩÎ°ú
  vocabulary_file, # Ïñ¥Ìúò ÏÇ¨Ï†Ñ ÌååÏùº Í≤ΩÎ°ú
  pretrained_file, # ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Î™®Îç∏ ÌååÏùº Í≤ΩÎ°ú
  device_ids,
  epochs,
  batch_size,
  save_dir,
  parallel=False,
  load_range=None
):
  # tokenizer, model, optimizer, scheduler
  tokenizer = AutoTokenizer.from_pretrained(vocabulary_file)
  model = CodeGenForCausalLM.from_pretrained(pretrained_file)
  print('üß† Model parameters:', sum(param.numel() for param in model.parameters()))
  # parallelÏù¥ FalseÏù∏ Í≤ΩÏö∞ DataParallelÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ modelÏùÑ Î≥ëÎ†¨Ìôî
  if not parallel:
    model = nn.DataParallel(model, device_ids=device_ids).to(device_ids[0])
  # parallelÏù¥ TrueÏù∏ Í≤ΩÏö∞ 
  else:
    # ÌòÑÏû¨ Íµ¨ÌòÑÎêòÏßÄ ÏïäÏùå. ÏòàÏô∏ Î∞òÌôòÌïòÍ≥† Ï¢ÖÎ£å
    raise NotImplementedError('Parallel training is not implemented yet.')
    # if 'codegen-350M' in pretrained_file:
    #   model.parallelize(device_map = {
    #     0: [_ for _ in range(0, 20)]
    #   })
    # elif 'codegen-2B' in pretrained_file:
    #   model.parallelize(device_map = {
    #     0: [_ for _ in range(0, 7)],
    #     1: [_ for _ in range(7, 16)],
    #     2: [_ for _ in range(16, 25)],
    #     3: [_ for _ in range(25, 32)]
    #   })
    # else:
    #   model.parallelize(device_map = {
    #     0: [_ for _ in range(0, 4)], 
    #     1: [_ for _ in range(4, 8)],
    #     2: [_ for _ in range(8, 12)],
    #     3: [_ for _ in range(12, 16)],
    #     4: [_ for _ in range(16, 20)],
    #     5: [_ for _ in range(20, 24)],
    #     6: [_ for _ in range(24, 29)],
    #     7: [_ for _ in range(29, 33)]
    #   })
  
  # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã, Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú
  # CodeLlamaÏóêÏÑúÎäî Î©îÎ™®Î¶¨ÏôÄ ÏÜçÎèÑ Î¨∏Ï†úÎ°ú max_lengthÎ•º 768Î°ú ÏÑ§Ï†ï
  train_dataset = Dataset(
    training_file,
    tokenizer,
    max_length=768,
    shuffle=False,
    load_range=load_range
  )
  validation_dataset = Dataset(
    validation_file,
    tokenizer,
    max_length=768,
    load_range=None
  )

  training_sampler = torch.utils.data.SequentialSampler(train_dataset)
  validation_sampler = torch.utils.data.SequentialSampler(validation_dataset)

  # Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±
  train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=0, # Îç∞Ïù¥ÌÑ∞ Î°úÎî©ÏùÑ ÏúÑÌïú ÌîÑÎ°úÏÑ∏Ïä§ Ïàò
    pin_memory=True, # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î•º Í≥†Ï†ï Î©îÎ™®Î¶¨Ïóê Ïò¨Î¶º
    sampler=training_sampler, # Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïãú ÏÇ¨Ïö©Ìï† ÏÉòÌîåÎü¨
    collate_fn=custom_collate, # Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïãú ÏÇ¨Ïö©Ìï† Ìï®Ïàò
  )
  validation_loader = torch.utils.data.DataLoader(
    dataset=validation_dataset,
    batch_size=3*batch_size, # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Î∞∞Ïπò ÌÅ¨Í∏∞Îäî ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò 3Î∞∞
    shuffle=False,
    num_workers=0,
    pin_memory=True,
    sampler=validation_sampler,
    collate_fn=custom_collate,
  )

  # optimizerÏôÄ scheduler ÏÑ§Ï†ï
  optimizer = Adafactor(
    model.parameters(),
    lr=1e-5, # ÌïôÏäµÎ•†
    scale_parameter=False, # üçû
    relative_step=False # üçû
  )
  scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=int(epochs * len(train_loader))
  )

  # tqdmÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô©ÏùÑ Ï∂úÎ†•
  for epoch in range(epochs):
    model.train()
    training_loss = []
    start_time = time.time()
    oom = 0
    print('üöÄ Epoch start:', epoch + 1, '/', epochs)
    for i, data in enumerate(tqdm.tqdm(train_loader, total=len(train_loader))):
      data = {
        'input_ids': data['input_ids'].to(device_ids[0]),
        'labels': data['labels'].to(device_ids[0]),
        'attention_mask': data['attention_mask'].to(device_ids[0])
      }
      try:
        optimizer.zero_grad()
        output = model(
          input_ids=data['input_ids'],
          labels=data['labels'],
          attention_mask=data['attention_mask'],
          return_dict=True
        )
        loss = output.loss
        # lossÏùò ÌèâÍ∑†ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ training_lossÏóê Ï∂îÍ∞Ä
        training_loss.append(loss.mean().item())
        loss.mean().backward() # Ïó≠Ï†ÑÌåå
        optimizer.step()
        scheduler.step()
      except RuntimeError as e:
        if 'out of memory' in str(e):
          oom += 1
          print(f'üí• Out of Memory: {oom}')
          model.zero_grad()
          optimizer.zero_grad()
          scheduler.step()
          del data

          torch.cuda.empty_cache()
          time.sleep(5)
        else:
          print('üí• RuntimeError:', e)
          traceback.print_exc()
          break
      
      if i % 1000 == 0:
        # Î°úÍ∑∏ Ï∂úÎ†•
        print('üìù Epoch: {}, üë£ step: {}/{}, üî• loss: {}, üìà lr: {}, üí• oom: {}, ‚åõ time: {}s'.format(
          epoch + 1, i, len(train_loader),
          round(sum(training_loss) / len(training_loss), 4),
          round(scheduler.get_last_lr()[0], 7),
          oom,
          int(time.time() - start_time)
        ))
        start_time = time.time()
        oom = 0
      if i % 10000 == 0 and i > 0:
        # Í≤ÄÏ¶ù Í≥ºÏ†ï Ïã§Ìñâ
        step_validation(model, device_ids, validation_loader, save_dir, parallel=parallel)
    # ÏµúÏ¢Ö Í≤ÄÏ¶ù Í≥ºÏ†ï Ïã§Ìñâ
    step_validation(model, device_ids, validation_loader, save_dir, parallel)

if __name__ == '__main__':
  device_ids = [0]
  training_file = ''      # fine-tuning Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω
  validation_file = ''    # fine-tuning Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω
  vocabulary_file = 'CodeLlama-7b-hf/'
  pretrained_file = 'CodeLlama-7b-hf/'

  run_finetune(
    training_file,
    validation_file,
    vocabulary_file,
    pretrained_file,
    device_ids,
    epochs=1,
    batch_size=1,
    save_dir='../../outputs/CodeLlama-7b-hf',
    parallel=False,
    load_range=None
  )